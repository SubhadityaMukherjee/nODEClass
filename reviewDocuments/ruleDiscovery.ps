%!PS-Adobe-3.0
%%Creator: groff version 1.22.4
%%CreationDate: Fri Mar 12 01:37:44 2021
%%DocumentNeededResources: font Times-Bold
%%+ font Times-Italic
%%+ font Times-Roman
%%DocumentSuppliedResources: procset grops 1.22 4
%%Pages: 5
%%PageOrder: Ascend
%%DocumentMedia: Default 595 842 0 () ()
%%Orientation: Portrait
%%EndComments
%%BeginDefaults
%%PageMedia: Default
%%EndDefaults
%%BeginProlog
%%BeginResource: procset grops 1.22 4
%!PS-Adobe-3.0 Resource-ProcSet
/setpacking where{
pop
currentpacking
true setpacking
}if
/grops 120 dict dup begin
/SC 32 def
/A/show load def
/B{0 SC 3 -1 roll widthshow}bind def
/C{0 exch ashow}bind def
/D{0 exch 0 SC 5 2 roll awidthshow}bind def
/E{0 rmoveto show}bind def
/F{0 rmoveto 0 SC 3 -1 roll widthshow}bind def
/G{0 rmoveto 0 exch ashow}bind def
/H{0 rmoveto 0 exch 0 SC 5 2 roll awidthshow}bind def
/I{0 exch rmoveto show}bind def
/J{0 exch rmoveto 0 SC 3 -1 roll widthshow}bind def
/K{0 exch rmoveto 0 exch ashow}bind def
/L{0 exch rmoveto 0 exch 0 SC 5 2 roll awidthshow}bind def
/M{rmoveto show}bind def
/N{rmoveto 0 SC 3 -1 roll widthshow}bind def
/O{rmoveto 0 exch ashow}bind def
/P{rmoveto 0 exch 0 SC 5 2 roll awidthshow}bind def
/Q{moveto show}bind def
/R{moveto 0 SC 3 -1 roll widthshow}bind def
/S{moveto 0 exch ashow}bind def
/T{moveto 0 exch 0 SC 5 2 roll awidthshow}bind def
/SF{
findfont exch
[exch dup 0 exch 0 exch neg 0 0]makefont
dup setfont
[exch/setfont cvx]cvx bind def
}bind def
/MF{
findfont
[5 2 roll
0 3 1 roll
neg 0 0]makefont
dup setfont
[exch/setfont cvx]cvx bind def
}bind def
/level0 0 def
/RES 0 def
/PL 0 def
/LS 0 def
/MANUAL{
statusdict begin/manualfeed true store end
}bind def
/PLG{
gsave newpath clippath pathbbox grestore
exch pop add exch pop
}bind def
/BP{
/level0 save def
1 setlinecap
1 setlinejoin
DEFS/BPhook known{DEFS begin BPhook end}if
72 RES div dup scale
LS{
90 rotate
}{
0 PL translate
}ifelse
1 -1 scale
}bind def
/EP{
level0 restore
showpage
}def
/DA{
newpath arcn stroke
}bind def
/SN{
transform
.25 sub exch .25 sub exch
round .25 add exch round .25 add exch
itransform
}bind def
/DL{
SN
moveto
SN
lineto stroke
}bind def
/DC{
newpath 0 360 arc closepath
}bind def
/TM matrix def
/DE{
TM currentmatrix pop
translate scale newpath 0 0 .5 0 360 arc closepath
TM setmatrix
}bind def
/RC/rcurveto load def
/RL/rlineto load def
/ST/stroke load def
/MT/moveto load def
/CL/closepath load def
/Fr{
setrgbcolor fill
}bind def
/setcmykcolor where{
pop
/Fk{
setcmykcolor fill
}bind def
}if
/Fg{
setgray fill
}bind def
/FL/fill load def
/LW/setlinewidth load def
/Cr/setrgbcolor load def
/setcmykcolor where{
pop
/Ck/setcmykcolor load def
}if
/Cg/setgray load def
/RE{
findfont
dup maxlength 1 index/FontName known not{1 add}if dict begin
{
1 index/FID ne
2 index/UniqueID ne
and
{def}{pop pop}ifelse
}forall
/Encoding exch def
dup/FontName exch def
currentdict end definefont pop
}bind def
/DEFS 0 def
/EBEGIN{
moveto
DEFS begin
}bind def
/EEND/end load def
/CNT 0 def
/level1 0 def
/PBEGIN{
/level1 save def
translate
div 3 1 roll div exch scale
neg exch neg exch translate
0 setgray
0 setlinecap
1 setlinewidth
0 setlinejoin
10 setmiterlimit
[]0 setdash
/setstrokeadjust where{
pop
false setstrokeadjust
}if
/setoverprint where{
pop
false setoverprint
}if
newpath
/CNT countdictstack def
userdict begin
/showpage{}def
/setpagedevice{}def
mark
}bind def
/PEND{
cleartomark
countdictstack CNT sub{end}repeat
level1 restore
}bind def
end def
/setpacking where{
pop
setpacking
}if
%%EndResource
%%EndProlog
%%BeginSetup
%%BeginFeature: *PageSize Default
<< /PageSize [ 595 842 ] /ImagingBBox null >> setpagedevice
%%EndFeature
%%IncludeResource: font Times-Bold
%%IncludeResource: font Times-Italic
%%IncludeResource: font Times-Roman
grops begin/DEFS 1 dict def DEFS begin/u{.001 mul}bind def end/RES 72
def/PL 841.89 def/LS false def/ENC0[/asciicircum/asciitilde/Scaron
/Zcaron/scaron/zcaron/Ydieresis/trademark/quotesingle/Euro/.notdef
/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef
/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef/.notdef
/.notdef/.notdef/.notdef/space/exclam/quotedbl/numbersign/dollar/percent
/ampersand/quoteright/parenleft/parenright/asterisk/plus/comma/hyphen
/period/slash/zero/one/two/three/four/five/six/seven/eight/nine/colon
/semicolon/less/equal/greater/question/at/A/B/C/D/E/F/G/H/I/J/K/L/M/N/O
/P/Q/R/S/T/U/V/W/X/Y/Z/bracketleft/backslash/bracketright/circumflex
/underscore/quoteleft/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y
/z/braceleft/bar/braceright/tilde/.notdef/quotesinglbase/guillemotleft
/guillemotright/bullet/florin/fraction/perthousand/dagger/daggerdbl
/endash/emdash/ff/fi/fl/ffi/ffl/dotlessi/dotlessj/grave/hungarumlaut
/dotaccent/breve/caron/ring/ogonek/quotedblleft/quotedblright/oe/lslash
/quotedblbase/OE/Lslash/.notdef/exclamdown/cent/sterling/currency/yen
/brokenbar/section/dieresis/copyright/ordfeminine/guilsinglleft
/logicalnot/minus/registered/macron/degree/plusminus/twosuperior
/threesuperior/acute/mu/paragraph/periodcentered/cedilla/onesuperior
/ordmasculine/guilsinglright/onequarter/onehalf/threequarters
/questiondown/Agrave/Aacute/Acircumflex/Atilde/Adieresis/Aring/AE
/Ccedilla/Egrave/Eacute/Ecircumflex/Edieresis/Igrave/Iacute/Icircumflex
/Idieresis/Eth/Ntilde/Ograve/Oacute/Ocircumflex/Otilde/Odieresis
/multiply/Oslash/Ugrave/Uacute/Ucircumflex/Udieresis/Yacute/Thorn
/germandbls/agrave/aacute/acircumflex/atilde/adieresis/aring/ae/ccedilla
/egrave/eacute/ecircumflex/edieresis/igrave/iacute/icircumflex/idieresis
/eth/ntilde/ograve/oacute/ocircumflex/otilde/odieresis/divide/oslash
/ugrave/uacute/ucircumflex/udieresis/yacute/thorn/ydieresis]def
/Times-Roman@0 ENC0/Times-Roman RE/Times-Italic@0 ENC0/Times-Italic RE
/Times-Bold@0 ENC0/Times-Bold RE
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
BP
%%EndPageSetup
/F0 12/Times-Bold@0 SF(Rule Disco)145.614 123 Q -.12(ve)-.12 G
(ry using Neural ODEs - ASL Classi\214cation).12 E/F1 10/Times-Italic@0
SF(Subhaditya Mukherjee \(17BCE2193\))213.845 159 Q(Supr)225.855 171 Q
(atim Sarkar \(17BCE2203\))-.15 E/F2 10/Times-Bold@0 SF 2.5(1. Guide)72
219 R/F3 10/Times-Roman@0 SF(Professor V)72 234.6 Q
(ijaya Rajan V will be guiding us through our project.)-.6 E F2 2.5
(2. Resear)72 270.6 R(ch Pr)-.18 E(oposal)-.18 E F3 .727(In an)72 298.2
R 3.227<798c>-.15 G .727(eld, data modeling is an e)-3.227 F .727
(xtremely important f)-.15 F .727
(actor in understanding a system. In classical cases,)-.1 F .23
(Ordinary dif)72 310.2 R .23
(ferential equations were used to perform this disco)-.25 F -.15(ve)-.15
G(ry).15 E 2.73(.Ar)-.65 G .23(ecent paper[1] mer)-2.73 F .23
(ged the concepts)-.18 F .862(of ODEs with Deep learning and created a \
neural ODE in which a ResNet architecture')72 322.2 R 3.361(sr)-.55 G
.861(es-blocks were)-3.361 F .381(substituted with an ODE solv)72 334.2
R .382(er which led to not only better results, b)-.15 F .382
(ut also more memory ef)-.2 F(\214cienc)-.25 E 1.682 -.65(y. T)-.15 H
(his).65 E .18
(project aims to attempt to come up with a better architecture that gi)
72 346.2 R -.15(ve)-.25 G 2.679(sS).15 G -.4(OT)-2.679 G 2.679(Ar)-.53 G
.179(esults on the ASL classi\214ca-)-2.679 F(tion dataset.)72 358.2 Q
F2 2.5(3. W)72 394.2 R(ork\215o)-.75 E(w)-.1 E F3 .67
(The \214rst part of the project will in)72 409.8 R -.2(vo)-.4 G(lv).2 E
3.17(eg)-.15 G .67(oing through multiple papers and doing a comprehensi)
-3.17 F .97 -.15(ve s)-.25 H(urv).15 E -.15(ey)-.15 G .468(of the w)72
421.8 R .468(ork that is already there. The objecti)-.1 F .768 -.15
(ve i)-.25 H 2.968(st).15 G 2.968(oi)-2.968 G .467
(dentify which architectures are being used and possi-)-2.968 F 1.367
(bly what memory requirements the)72 433.8 R 3.868(yc)-.15 G 1.368
(ome with in comparison to their non ODE counterparts. Standard)-3.868 F
.284(metrics will be used to compare the architectures and possibly oth\
er metrics to identify ho)72 445.8 R 2.783(ww)-.25 G .283
(ell the archi-)-2.783 F(tectures generalize.)72 457.8 Q .316
(The second part of the project will in)72 485.4 R -.2(vo)-.4 G(lv).2 E
2.816(et)-.15 G .316
(esting multiple architectures on classi\214cation tasks and coming)
-2.816 F 1.367(up with one that does better on a v)72 497.4 R 1.366
(ariety of these tasks and is more suited to being used with an ODE)-.25
F(solv)72 509.4 Q(er)-.15 E 5.07(.W)-.55 G 2.57(ew)-5.87 G .071(ill sta\
rt with a standard ResNet architecture and then branch out to similar o\
nes. Since this is a)-2.57 F(relati)72 521.4 Q -.15(ve)-.25 G .5(ly ne)
.15 F 3(wp)-.25 G(aper)-3 E 3(,t)-.4 G .5(here are man)-3 F 3(ya)-.15 G
.5(rchitectures that ha)-3 F .799 -.15(ve n)-.2 H .499(ot been w).15 F
(ork)-.1 E .499(ed on and those will be tar)-.1 F(geted)-.18 E .108(\
\214rst. The insights from these will be used to come up with a better \
architecture and \214nd out what in\215uences)72 533.4 R(it.)72 545.4 Q
F2 2.5(4. System)72 581.4 R F3(Since this is a Deep learning e)72 597 Q
(xperiment, a system with a GPU will be used.)-.15 E
(OS - Linux \(Arch\))72 621 Q(Language - Python)72 645 Q F2 2.5
(5. Libraries)72 681 R(that will be used)2.5 E F3 15(1. nump)97 708.6 R
(y)-.1 E 0 Cg EP
%%Page: 2 2
%%BeginPageSetup
BP
%%EndPageSetup
/F0 10/Times-Roman@0 SF(-2-)282.17 48 Q 15(2. matplotlib)97 84 R 15
(3. p)97 99.6 R(ytorch)-.1 E 15(4. p)97 115.2 R(ytorch lightning)-.1 E
15(5. torchdif)97 130.8 R(feq)-.25 E/F1 10/Times-Bold@0 SF 2.5
(6. Literatur)72 154.8 R 2.5(eS)-.18 G(ur)-2.5 E -.1(ve)-.1 G(y).1 E F0
.57(Neural Netw)72 170.4 R .57(orks are important for the approximation\
 of bounded continuous functions in the \214eld of Ma-)-.1 F .739
(chine Learning. Such netw)72 182.4 R .739(orks pro)-.1 F .739(vide v)
-.15 F .739(arious frame)-.25 F -.1(wo)-.25 G .739
(rks and procedures for solving ODE\342 and PDE\342s).1 F .017
(in a numerical manner)72 194.4 R 2.517(.T)-.55 G .017
(he paper[9] discusses about the use of a solv)-2.517 F .016
(er for ODE\342s and PDE\342s using Neural)-.15 F(Netw)72 206.4 Q 1.608
(ork frame)-.1 F -.1(wo)-.25 G 1.608(rks. Such Neural Netw).1 F 1.608
(orks use function approximation for performing the tasks. The)-.1 F
(solv)72 218.4 Q 1.154(er has pro)-.15 F -.15(ve)-.15 G 3.654(nt).15 G
3.654(os)-3.654 G(ho)-3.654 E 3.654(wh)-.25 G 1.153(igh accurac)-3.654 F
3.653(yf)-.15 G 1.153(or both initial v)-3.653 F 1.153
(alue problems and boundary v)-.25 F 1.153(alue problems.it)-.25 F -.1
(wo)72 230.4 S 1.07(rks for the functions as well as the deri).1 F -.25
(va)-.25 G(ti).25 E -.15(ve)-.25 G 1.07(s. The solv).15 F 1.07
(er is tested using collocation points using the)-.15 F(Bur)72 242.4 Q
.229(gers equation and the heat equations. The ODE solv)-.18 F .229
(er w)-.15 F .229(orks well for the ODE\342s and sho)-.1 F .228
(ws great accu-)-.25 F(rac)72 254.4 Q 2.318 -.65(y. B)-.15 H 1.018
(oundary v).65 F 1.018(alue problems are challenging to solv)-.25 F
3.519(eu)-.15 G 1.019(sing the equation mentioned and another dra)-3.519
F(w-)-.15 E .541(back is the computational ef)72 266.4 R(\214cienc)-.25
E 3.041(yo)-.15 G 3.041(ft)-3.041 G .541(he ODE solv)-3.041 F(er)-.15 E
3.041(.A)-.55 G(ccurac)-3.041 E 3.041(ym)-.15 G .54(ust be impro)-3.041
F -.15(ve)-.15 G 3.04(di).15 G 3.04(nt)-3.04 G .54(he case of bound-)
-3.04 F .281(ary v)72 278.4 R .281(alue problems.)-.25 F .282
(The paper [5] discusses that the stochastic gradient for a gi)5.282 F
-.15(ve)-.25 G 2.782(nR).15 G .282(esNet neural netw)-2.782 F(ork)-.1 E
.032(is made to con)72 290.4 R -.15(ve)-.4 G -.18(rg).15 G 2.532(et).18
G 2.532(ot)-2.532 G .031(he stochastic gradient descent for a Neural OD\
E. All the layers in the ResNet neural)-2.532 F(netw)72 302.4 Q .123
(ork share the same weighted matrix. It pro)-.1 F -.15(ve)-.15 G 2.623
(st).15 G .124
(hat Neural ODE\342s are the deep limit of such neural net-)-2.623 F -.1
(wo)72 314.4 S 1.226(rks. Se).1 F -.15(ve)-.25 G 1.226(ral equations ha)
.15 F 1.526 -.15(ve b)-.2 H 1.225
(een tried and the result has been obtained using the F).15 F(okk)-.15 E
(er)-.1 E 1.225(-Plank equa-)-.2 F .212
(tions\342 equation and theorems resulted in a lar)72 326.4 R .213
(ge number of con)-.18 F -.15(ve)-.4 G -.18(rg).15 G .213
(ence results. Con).18 F -.15(ve)-.4 G -.18(rg).15 G .213
(ence rates are high).18 F .609
(and are optimal for the results. It only considered the con)72 338.4 R
-.15(ve)-.4 G -.18(rg).15 G .608
(ence of minimizers and not the optimization).18 F .2(procedure. Re)72
350.4 R .2(gularization cannot solely e)-.15 F .201
(xplain the numerical simulation.)-.15 F .201(The use of Dif)5.201 F
.201(fEqFlux library is)-.25 F .454(described in [10] and an e)72 362.4
R .454(xplanation for ho)-.15 F 2.954(wi)-.25 G 2.954(ti)-2.954 G 2.954
(su)-2.954 G .454(sed for solving se)-2.954 F -.15(ve)-.25 G .453
(ral ODE\342s and other type of dif).15 F(fer)-.25 E(-)-.2 E .703
(ential equations is gi)72 374.4 R -.15(ve)-.25 G .704
(n. All the ODE\342s are tak).15 F .704
(en as a \215ux de\214ned neural netw)-.1 F .704
(ork. The purpose of using)-.1 F .674(the Dif)72 386.4 R .674
(fEqFlux library is to solv)-.25 F 3.174(es)-.15 G .674
(uch ODE\342s where simple inte)-3.174 F .674(gration strate)-.15 F .673
(gies are not suf)-.15 F .673(\214cient to get)-.25 F .169
(optimal results and solutions with high accurac)72 398.4 R 1.469 -.65
(y. T)-.15 H .169(he ODE\342s are de\214ned as neural netw).65 F .17
(orks and proper de-)-.1 F 1.126(scription of the Flux model zoo is pro)
72 410.4 R 1.126(vided that includes neural stochastic dif)-.15 F 1.126
(ferential equations. The li-)-.25 F .283(brary helps in de)72 422.4 R
-.15(ve)-.25 G .283(loping highly accurate ODE solv).15 F .284
(ers that can play a major role in v)-.15 F .284(arious applications of)
-.25 F .235(Machine Learning and Data Science.)72 434.4 R -1.11(Va)5.235
G .235(rious architectures of Deep Learning ha)1.11 F .535 -.15(ve n)-.2
H .735 -.25(ow b).15 H .235(een considered as).25 F .24
(Neural ODE\342s. It helps bring closer the g)72 446.4 R .241
(ap between deep learning and systems that are dynamic in nature.)-.05 F
.445(The major challenges f)72 458.4 R .445
(aced by [4] are \214guring out the w)-.1 F .444
(orking of the models and the dynamics associated)-.1 F .354
(with them. The design choices for b)72 470.4 R .354
(uilding the modules ha)-.2 F .654 -.15(ve t)-.2 H 2.854(ob).15 G 2.854
(ee)-2.854 G .354(lucidiated.The frame)-2.854 F -.1(wo)-.25 G .354
(rk is set up for).1 F -.2(bu)72 482.4 S .499(ilding a general Neural O\
DE and the components associated with it are considered. In\214nite dim\
ensional).2 F 1.287(problem has been solv)72 494.4 R 1.288(ed using num\
erical approximations that lead to better models for solving ODE\342s.)
-.15 F .058(There are a v)72 506.4 R .058(ariety of linear Ordinary dif)
-.25 F .057(ferential equations that e)-.25 F .057
(xist today for which v)-.15 F .057(arious methods e)-.25 F(xist)-.15 E
.681(to solv)72 518.4 R 3.181(et)-.15 G .682(hem. The paper[3] discusse\
s about the multiquadric radial basis function netw)-3.181 F .682
(orks and its use in)-.1 F .028(solving linear ODE\342s in an ef)72
530.4 R .028(\214cient manner)-.25 F 2.528(.B)-.55 G 2.528(ya)-2.528 G
.028(pproximating functions and its deri)-2.528 F -.25(va)-.25 G(ti).25
E -.15(ve)-.25 G 2.528(su).15 G .028(sing radial ba-)-2.528 F .271
(sis function netw)72 542.4 R .272
(orks, another RBFN is formed which can be ef)-.1 F .272
(\214cient in solving Linear ODE\342s with better)-.25 F(accurac)72
554.4 Q 2.352 -.65(y. I)-.15 H 3.552(ti).65 G 3.552(st)-3.552 G 1.051
(he most optimal method to determine the e)-3.552 F 1.051
(xisting parameters. It \214x)-.15 F 1.051(es the problem of not)-.15 F
(ha)72 566.4 Q .844
(ving enough data samples as it deals with only the domain and the e)-.2
F .844(xisting boundaries associated with)-.15 F .268
(it.The method can be used to solv)72 578.4 R 2.768(ee)-.15 G -.15(ve)
-3.018 G 2.768(nP).15 G .268
(DE\342s.A sample set is selected from the training set and positions)
-2.768 F .139(of centers of basis functions are selected-means clusteri\
ng algorithm is used on the training set and the cen-)72 590.4 R .792(t\
ers of the clusters are used as the centers of the basis functions. It \
is useful for lar)72 602.4 R .792(ge-scale problems that)-.18 F -.15(ex)
72 614.4 S .847(ist in se).15 F -.15(ve)-.25 G .847
(ral engineering \214elds.The major challenge f).15 F .848
(aced by this approach is that the number of basis)-.1 F .15
(fucntions and data set must be pro)72 626.4 R .15(vided with a priori.)
-.15 F .15(Function approximation has al)5.15 F -.1(wa)-.1 G .15
(ys been an unstable).1 F .144(task when it comes to using dif)72 638.4
R .145(ferential equations. The challenge of achie)-.25 F .145
(ving a stable \215o)-.25 F 2.645(wa)-.25 G .145(nd also ha)-2.645 F
(ving)-.2 E 2.964(al)72 650.4 S .964 -.25(ow c)-2.964 H .463(omputation\
al cost has been tackled by the authors[8]. The techniques proposed in \
this paper use an).25 F(ener)72 662.4 Q .943(gy function to guarantee t\
he asymptomatic stability of the solution. Ev)-.18 F .944
(ery deep learning algorithm is)-.15 F .284(function composition betwee\
n an input and an output space and is basically a mapping between them.\
 This)72 674.4 R .002(process w)72 686.4 R .002(as kno)-.1 F .002(wn b)
-.25 F .002(ut a proper optimization process w)-.2 F .002
(as not identi\214ed so f)-.1 F(ar)-.1 E 2.503(.T)-.55 G .003
(his paper proposes an algo-)-2.503 F .8
(rithm to reduce the instability and stif)72 698.4 R .799
(fness that a solv)-.25 F .799(er might f)-.15 F .799
(ace by using a parametrized ener)-.1 F .799(gy func-)-.18 F .473
(tion. This tak)72 710.4 R .473(es care of of the \215oor of the netw)
-.1 F .474(ork States and mo)-.1 F -.15(ve)-.15 G 2.974(si).15 G 2.974
(ta)-2.974 G .474(long the ne)-2.974 F -.05(ga)-.15 G(ti).05 E .774 -.15
(ve g)-.25 H .474(radient. Instead).15 F .239(of trying to learn a sing\
le function, the algorithm tries to learn functions from the function s\
pace. There are)72 722.4 R 0 Cg EP
%%Page: 3 3
%%BeginPageSetup
BP
%%EndPageSetup
/F0 10/Times-Roman@0 SF(-3-)282.17 48 Q .567
(multiple types of models that are possible and observ)72 84 R .567
(ed. Autonomous port Hamiltonian model, second or)-.15 F(-)-.2 E .166(d\
er and stochastic models are also de\214ned. In training the model smoo\
th scaler cost function w)72 96 R .166(as used to to)-.1 F .493
(understand ho)72 108 R 2.993(ww)-.25 G .494(ell the model performs and\
 also to minimize the loss in an optimal setting. The comput-)-2.993 F
1.227(ing of gradients is is ef)72 120 R 1.227
(\214cient with respect to to the choice of the parameters. T)-.25 F
3.727(oe)-.8 G 1.226(nsure that the ener)-3.727 F(gy)-.18 E .945
(function remains steady)72 132 R 3.445(,as)-.65 G .945
(oft constraint w)-3.445 F .945(as also added.)-.1 F .945(Neural netw)
5.945 F .945(ork architectures are composed of)-.1 F 1.084(states as we\
ll as hidden layers. Using this concept, it is possible to understand t\
he mapping between the)72 144 R -.15(ve)72 156 S .049
(ctor spaces of the input and outputs. W).15 F 2.549(ec)-.8 G .049
(an further tak)-2.549 F 2.549(et)-.1 G .049
(his idea and replace the entire component of the)-2.549 F .022
(hidden states by using a dif)72 168 R .021(ferential equation solv)-.25
F(er)-.15 E 2.521(.A)-.55 G .021(nd framing the netw)-2.521 F .021
(ork problem as an ODE problem.)-.1 F .879
(This is the premise of a paper by Chen et al.[6] The major adv)72 180 R
.88(antage of using models success this is the)-.25 F .306(near constan\
t memory costs as well as a higher customization by means of allo)72 192
R -.1(wa)-.25 G .306(nce for trade of).1 F 2.805(fb)-.25 G(etween)-2.805
E 1.662(speed as well as precision v)72 204 R 1.662
(alues. The second adv)-.25 F 1.662(antage is that of huge boost in ef)
-.25 F(\214cienc)-.25 E 4.163(yd)-.15 G 1.663(ue to the)-4.163 F .454
(process of back propag)72 216 R .453(ation not being required as a dif)
-.05 F .453(ferential equation solv)-.25 F .453
(er is being used instead. This)-.15 F 1.194(also leads to pretty high \
guarantees about stagnating errors as well as the ability to be trained\
 using the)72 228 R .612(maximum lik)72 240 R .611(elihood model. It is\
 also possible to to train on data that is not static and arri)-.1 F
-.15(ve)-.25 G 3.111(sn).15 G .611(on concur)-3.111 F(-)-.2 E .122
(rently due to its nature. The method used to solv)72 252 R 2.622(et)
-.15 G .122(he equations mak)-2.622 F .122
(es use of the calculation of a secondary)-.1 F .412(equation backw)72
264 R .412(ards which allo)-.1 F .412(ws for ef)-.25 F(\214cienc)-.25 E
2.912(yi)-.15 G 2.912(nm)-2.912 G .412
(emory as well as the ability to de\214ne the error thresh-)-2.912 F
1.031(old. These bene\214ts do not generally come with standard deep le\
arning models. Making use of this black)72 276 R .034(box idea, the aut\
hors replace the standard ResNet models with an architecture that uses \
the solv)72 288 R .033(er instead of)-.15 F .427(hidden layers. The onl\
y issue is that the analogue to the depth of a netw)72 300 R .428
(ork cannot be obtained in this sce-)-.1 F .641(nario. The model RK-Net\
 performs similar to the original ResNet on the MNIST dataset and man)72
312 R 3.141(yo)-.15 G(thers)-3.141 E .092(as described in the paper)72
324 R 2.592(.U)-.55 G .092(sing a solv)-2.592 F .092(er also allo)-.15 F
.092(ws the model to ha)-.25 F .392 -.15(ve m)-.2 H .093
(ultiple hidden units without af).15 F(fect-)-.25 E .195
(ing the time comple)72 336 R .195(xity of the training.)-.15 F .195
(Being able to disco)5.195 F -.15(ve)-.15 G 2.695(rn).15 G .695 -.25
(ew e)-2.695 H .195(quations using data has al).25 F -.1(wa)-.1 G .195
(ys been a).1 F .423(dream when it comes to the \214eld of computer sci\
ence. A recent paper [1] sho)72 348 R .423
(ws the possibility of harness-)-.25 F .415(ing deep learning for the s\
ame by augmenting the capabilities of scienti\214c modeling. This leads\
 to the cre-)72 360 R .354(ation of an UDE which w)72 372 R .354
(ould potentially enable the user to e)-.1 F .354(xtrapolate f)-.15 F
.354(actors that do not e)-.1 F .355(xist in the data)-.15 F .306
(as well as be able to simulate better models more accurately)72 384 R
2.806(.U)-.65 G .306(sing the principles of ph)-2.806 F .305
(ysics, it is also pos-)-.05 F .735(sible to incorporate the e)72 396 R
.736(xisting kno)-.15 F .736(wledge into the netw)-.25 F .736
(ork. The combination of dif)-.1 F .736(ferential equations in)-.25 F
.379(an ML concept to create a PINN using the U)72 408 R 2.598 -1.11
(AT h)-.4 H .378(as also been sho)1.11 F .378
(wn. The paper [2] demonstrates the abil-)-.25 F 1.333(ity to obtain th\
e equations that de\214ne a system using just data and also the possibi\
lity of reco)72 420 R -.15(ve)-.15 G 1.333(ring an).15 F .788
(equation from the time series data to e)72 432 R .787
(xtrapolate it. It also talks about the possibility of using the ph)-.15
F(ysics)-.05 E(la)72 444 Q 1.548(ws of conserv)-.15 F 1.548
(ation to to identify ne)-.25 F 4.048(ws)-.25 G 1.548
(ystems by means of transferring pre)-4.048 F 1.548(vious kno)-.25 F
1.548(wledge. The ap-)-.25 F .112(proach of using ph)72 456 R .111
(ysics and form netw)-.05 F .111(orks allo)-.1 F .111
(ws the creation of solv)-.25 F .111(ers that can ef)-.15 F .111
(\214ciently create gradient)-.25 F .296
(related calculations that are required to to perform an)72 468 R 2.796
(yk)-.15 G .297(ind of machine learning task.)-2.796 F(Ev)5.297 E .297
(en though the e)-.15 F(x-)-.15 E .95(istence of residual netw)72 480 R
.95(orks ha)-.1 F 1.25 -.15(ve b)-.2 H .95
(een present for a long time the the technique for or constructing such)
.15 F .248(models for a continuous and in)72 492 R -.15(ve)-.4 G .248
(rtible function w).15 F .248(as not present. This paper[7] sho)-.1 F
.248(ws ho)-.25 F 2.749(wt)-.25 G 2.749(op)-2.749 G(ro)-2.749 E .549
-.15(ve a)-.15 H .549 -.15(ny h).15 H(o-).15 E .051(meomorphism that ca\
n be approximated by using a combination of neural netw)72 504 R .051
(orks and ODEs in Euclidean)-.1 F 1.093(space. This leads to the abilit\
y of using residual connections and blocks to create a re)72 516 R -.15
(ve)-.25 G 1.094(rse mapping be-).15 F 1.265
(tween the output space as well as input space and allo)72 528 R 3.765
(wf)-.25 G 1.264(or applications such as deep learning oriented)-3.765 F
(ones.)72 540 Q/F1 10/Times-Bold@0 SF 2.5(7. Components)72 576 R
(that will be used)2.5 E 2.5(7.1. Dataset)72 612 R F0 1.457
(The dataset that will be used is the ASL dataset from https://www)72
627.6 R(.kaggle.com/grassknoted/asl-alphabet.)-.65 E(This has about 29 \
classes of which, 10 classes will initally be used to test the system.)
72 639.6 Q F1 2.5(7.2. Reading)72 675.6 R(the data)2.5 E F0 .139(This w\
ill be done using pandas and a dataframe will be created from the datas\
et to allo)72 691.2 R 2.638(wf)-.25 G .138(or easier process-)-2.638 F
.255(ing. Stratify will also be used to ensure that the classes remain \
the same count while sending it to the train-)72 703.2 R(ing loop.)72
715.2 Q 0 Cg EP
%%Page: 4 4
%%BeginPageSetup
BP
%%EndPageSetup
/F0 10/Times-Roman@0 SF(-4-)282.17 48 Q/F1 10/Times-Bold@0 SF 2.5
(7.3. T)72 84 R(raining details)-.74 E F0(These ha)72 99.6 Q .3 -.15
(ve n)-.2 H(ot been \214nalized b).15 E(ut an inital system in)-.2 E -.2
(vo)-.4 G(lv).2 E(es the follo)-.15 E(wing.)-.25 E 15(1. Sa)72 115.2 R
.3 -.15(ve h)-.2 H(yperparameters).1 E 15(2. Learning)72 130.8 R
(rate of 1e-14)2.5 E 15(3. W)72 146.4 R(eight decay of 0.0001)-.8 E 15
(4. Adam)72 162 R(Scheduler with def)2.5 E
(ault parameters and weight decay)-.1 E 15(5. Step)72 177.6 R
(Learning rate scheduling.)2.5 E 15(6. Loss)72 193.2 R
(function of cross entrop)2.5 E -.65(y.)-.1 G 15(7. Batch)72 208.8 R
(size of 128)2.5 E 15(8. Image)72 224.4 R(size of 64x64x3)2.5 E 15
(9. Float)72 240 R(16 precision)2.5 E 10(10. Distrib)72 255.6 R
(ution plugins with ddp and sharded plugins for single GPU parallelism)
-.2 E 10(11. Data)72 271.2 R(augmentations as follo)2.5 E(ws)-.25 E
(RandomResizedCrop\(img_size, img_size, p=1.0\),)97 295.2 Q -.35(Tr)97
319.2 S(anspose\(p=0.5\),).35 E(HorizontalFlip\(p=0.5\),)97 343.2 Q
-1.11(Ve)97 367.2 S(rticalFlip\(p=0.5\),)1.11 E
(ShiftScaleRotate\(p=0.5\),)97 391.2 Q(HueSaturationV)97 415.2 Q
(alue\(hue_shift_limit=0.2,sat_shift_limit=0.2,v)-1.11 E
(al_shift_limit=0.2,p=0.5\),)-.25 E(RandomBrightnessContrast\(brightnes\
s_limit=\(-0.1, 0.1\),contrast_limit=\(-0.1, 0.1\),p=0.5\),)97 439.2 Q(\
Normalize\(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225],max_pix)
97 463.2 Q(el_v)-.15 E(alue=255.0,p=1.0\),)-.25 E
(CoarseDropout\(p=0.5\),)97 487.2 Q(Cutout\(p=0.5\),)97 511.2 Q -.8(To)
97 535.2 S -.7(Te).8 G(nsorV2\(p=1.0\),).7 E F1 2.5(8. Refer)72 583.2 R
(ences)-.18 E F0 .389([1] Rackauckas, C., Ma, Y)72 610.8 R .388
(., Martensen, J., W)-1.29 F(arner)-.8 E 2.888(,C)-.4 G .388(., Zubo)
-2.888 F 1.688 -.65(v, K)-.15 H .388(., Supekar).65 F 2.888(,R)-.4 G
.388(., ... & Ramadhan, A. \(2020\).)-2.888 F(Uni)72 622.8 Q -.15(ve)
-.25 G(rsal dif).15 E
(ferential equations for scienti\214c machine learning. arXi)-.25 E 2.5
(vp)-.25 G(reprint arXi)-2.5 E(v:2001.04385.)-.25 E 1.533([2] Filici, C\
. \(2008\). On a neural approximator to ODEs. IEEE transactions on neur\
al netw)72 646.8 R 1.534(orks, 19\(3\),)-.1 F(539-543.)72 658.8 Q .424
([3] Jian)72 682.8 R .424(yu, L., Siwei, L., Y)-.15 F .424
(ingjian, Q., & Y)-.55 F .424
(aping, H. \(2003\). Numerical solution of elliptic partial dif)-1 F
(feren-)-.25 E(tial equation using radial basis function neural netw)72
694.8 Q(orks. Neural Netw)-.1 E(orks, 16\(5-6\), 729-734.)-.1 E .187
([4] Massaroli, S., Poli, M., P)72 718.8 R .187(ark, J., Y)-.15 F .187
(amashita, A., & Asama, H. \(2020\). Dissecting neural odes. arXi)-1 F
2.688(vp)-.25 G(re-)-2.688 E(print arXi)72 730.8 Q(v:2002.08071.)-.25 E
0 Cg EP
%%Page: 5 5
%%BeginPageSetup
BP
%%EndPageSetup
/F0 10/Times-Roman@0 SF(-5-)282.17 48 Q .593([5] A)72 84 R -.15(ve)-.74
G .592(lin, B., & Nystr\303\266m, K. \(2019\). Neural ODEs as the deep \
limit of ResNets with constant weights.).15 F(arXi)72 96 Q 2.5(vp)-.25 G
(reprint arXi)-2.5 E(v:1906.12183.)-.25 E .698([6] Chen, R. T)72 120 R
.698(., Rubano)-.74 F -.25(va)-.15 G 3.198(,Y).25 G .698
(., Bettencourt, J., & Duv)-4.488 F .698
(enaud, D. \(2018\). Neural ordinary dif)-.15 F .698(ferential equa-)
-.25 F(tions. arXi)72 132 Q 2.5(vp)-.25 G(reprint arXi)-2.5 E
(v:1806.07366.)-.25 E .208
([7] Zhang, H., Gao, X., Unterman, J., & Arodz, T)72 156 R 2.708(.\()
-.74 G .208(2020, No)-2.708 F -.15(ve)-.15 G .208
(mber\). Approximation capabilities of neural).15 F 4.97(ODEs and in)72
168 R -.15(ve)-.4 G 4.971(rtible residual netw).15 F 4.971
(orks. In International Conference on Machine Learning \(pp.)-.1 F
(11086-11095\). PMLR.)72 180 Q .004
([8] Massaroli, S., Poli, M., Bin, M., P)72 204 R .004(ark, J., Y)-.15 F
.004(amashita, A., & Asama, H. \(2020\). Stable neural \215o)-1 F .003
(ws. arXi)-.25 F(v)-.25 E(preprint arXi)72 216 Q(v:2003.08063.)-.25 E
.818([9] Liu, Z., Y)72 240 R .818(ang, Y)-1 F .818
(., & Cai, Q. \(2019\). Neural netw)-1.29 F .819
(ork as a function approximator and its application in)-.1 F
(solving dif)72 252 Q(ferential equations. Applied Mathematics and Mech\
anics, 40\(2\), 237-248.)-.25 E .222
([10] Rackauckas, C., Innes, M., Ma, Y)72 276 R .222
(., Bettencourt, J., White, L., & Dixit, V)-1.29 F 2.721(.\()-1.29 G
.221(2019\). Dif)-2.721 F .221(feq\215ux. jl-A julia)-.25 F
(library for neural dif)72 288 Q(ferential equations. arXi)-.25 E 2.5
(vp)-.25 G(reprint arXi)-2.5 E(v:1902.02376.)-.25 E 0 Cg EP
%%Trailer
end
%%EOF
